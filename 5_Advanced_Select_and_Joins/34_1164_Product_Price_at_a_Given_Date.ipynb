{"cells":[{"cell_type":"markdown","source":["# [1164. Product Price at a Given Date](https://leetcode.com/problems/product-price-at-a-given-date/description/?envType=study-plan-v2&envId=top-sql-50)"],"metadata":{"collapsed":false,"id":"31ae3b6c6405b34a"},"id":"31ae3b6c6405b34a"},{"cell_type":"markdown","source":["Table: Products\n","\n","<pre>+---------------+---------+\n","| Column Name   | Type    |\n","+---------------+---------+\n","| product_id    | int     |\n","| new_price     | int     |\n","| change_date   | date    |\n","+---------------+---------+</pre>\n","(product_id, change_date) is the primary key (combination of columns with unique values) of this table.\n","Each row of this table indicates that the price of some product was changed to a new price at some date.\n","\n","\n","Write a solution to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.\n","\n","Return the result table in any order.\n","\n","The result format is in the following example.\n","\n","\n","\n","Example 1:\n","\n","Input:\n","Products table:\n","<pre>+------------+-----------+-------------+\n","| product_id | new_price | change_date |\n","+------------+-----------+-------------+\n","| 1          | 20        | 2019-08-14  |\n","| 2          | 50        | 2019-08-14  |\n","| 1          | 30        | 2019-08-15  |\n","| 1          | 35        | 2019-08-16  |\n","| 2          | 65        | 2019-08-17  |\n","| 3          | 20        | 2019-08-18  |\n","+------------+-----------+-------------+</pre>\n","Output:\n","<pre>+------------+-------+\n","| product_id | price |\n","+------------+-------+\n","| 2          | 50    |\n","| 1          | 35    |\n","| 3          | 10    |\n","+------------+-------+</pre>"],"metadata":{"collapsed":false,"id":"e815efdad737e725"},"id":"e815efdad737e725"},{"cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---------+-------------------+\n","|product_id|new_price|change_date        |\n","+----------+---------+-------------------+\n","|1         |20       |2019-08-14 00:00:00|\n","|2         |50       |2019-08-14 00:00:00|\n","|1         |30       |2019-08-15 00:00:00|\n","|1         |35       |2019-08-16 00:00:00|\n","|2         |65       |2019-08-17 00:00:00|\n","|3         |20       |2019-08-18 00:00:00|\n","+----------+---------+-------------------+\n","\n"]}],"source":["# Pandas Schema\n","import pandas as pd\n","\n","data = [[1, 20, '2019-08-14'], [2, 50, '2019-08-14'], [1, 30, '2019-08-15'], [1, 35, '2019-08-16'],\n","        [2, 65, '2019-08-17'], [3, 20, '2019-08-18']]\n","\n","# data = [ #test case 4 from leet code\n","#     [1, 20, '2019-08-17'],\n","#     [2, 50, '2019-08-18'],\n","#     [1, 30, '2019-08-15'],\n","#     [1, 35, '2019-08-16'],\n","#     [2, 65, '2019-08-17'],\n","#     [3, 20, '2019-08-18']\n","# ]\n","\n","products = pd.DataFrame(data, columns=['product_id', 'new_price', 'change_date']).astype(\n","    {'product_id': 'Int64', 'new_price': 'Int64', 'change_date': 'datetime64[ns]'})\n","\n","\n","# to spark schema\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","products_df = spark.createDataFrame(products)\n","products_df.show(truncate=False)"],"metadata":{"ExecuteTime":{"end_time":"2023-11-05T19:23:47.711631400Z","start_time":"2023-11-05T19:23:47.056220Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"2cc89de269d42dfb","executionInfo":{"status":"ok","timestamp":1743290750634,"user_tz":-330,"elapsed":34203,"user":{"displayName":"Himanshu Singh","userId":"04013931203390313061"}},"outputId":"019b61ed-92ba-451b-f7ce-e9910ebe248c"},"id":"2cc89de269d42dfb"},{"cell_type":"code","source":["#In pyspark dataframe\n","\n","subdf = products_df\\\n","             .where(col('change_date') <= '2019-08-16')\\\n","             .select(\"product_id\").distinct()\n","\n","df1 = products_df\\\n","    .join(subdf, on=\"product_id\", how=\"left_anti\")\\\n","    .select(\"product_id\").distinct()\\\n","    .withColumn(\"price\", lit(10))\n","\n","\n","subdf=products_df\\\n","        .where(col('change_date') <= '2019-08-16')\\\n","        .groupBy('product_id').agg(max('change_date').alias('mx'))\n","\n","df2=products_df\\\n","        .join(subdf.alias('b'), (col('change_date')==col('mx')) & (products_df.product_id==subdf.product_id), 'inner')\\\n","        .select(('b.product_id'),col('new_price').alias('price'))\n","\n","\n","df1.union(df2).show()\n","\n","\n","# #note\n","# error you're encountering when using products_df['product_id'] or products_df.product_id in the select statement\n","# is due to the way PySpark handles column references, particularly in joins.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9CES1rIdLAS","executionInfo":{"status":"ok","timestamp":1743299502938,"user_tz":-330,"elapsed":3323,"user":{"displayName":"Himanshu Singh","userId":"04013931203390313061"}},"outputId":"aaf2d139-11f9-4d30-cfed-d6084a9ca33b"},"id":"N9CES1rIdLAS","execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+-----+\n","|product_id|price|\n","+----------+-----+\n","|         3|   10|\n","|         2|   50|\n","|         1|   35|\n","+----------+-----+\n","\n"]}]},{"cell_type":"code","source":["#In pyspark dataframe using subquery\n","\n","subdf = products_df\\\n","             .where(col('change_date') <= '2019-08-16')\\\n","             .select(\"product_id\").distinct()\n","#Collecting in list\n","valid_combinations = [row.product_id for row in subdf.collect()]\n","\n","\n","#compare using isin\n","df1 = products_df\\\n","    .where( ~col('product_id').isin(valid_combinations))\\\n","    .select(\"product_id\").distinct()\\\n","    .withColumn(\"price\", lit(10))\n","\n","\n","subdf=products_df\\\n","        .where(col('change_date') <= '2019-08-16')\\\n","        .groupBy('product_id').agg(max('change_date').alias('mx'))\n","\n","# Collect (product_id, max_change_date) into separate lists\n","valid_product_ids = [row.product_id for row in subdf.collect()]\n","valid_change_dates = [row.mx for row in subdf.collect()]\n","\n","# Filter using isin for product_id and change_date separately\n","dfx = products_df\\\n","    .where(col('product_id').isin(valid_product_ids) & col('change_date').isin(valid_change_dates))\\\n","    .select(('product_id'),col('new_price').alias('price'))\n","\n","df1.union(dfx).show()\n"],"metadata":{"id":"SOsNa9eAzcxX"},"id":"SOsNa9eAzcxX","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+-----+\n","|product_id|price|\n","+----------+-----+\n","|         3|   10|\n","|         2|   50|\n","|         1|   35|\n","+----------+-----+\n","\n"]}],"source":["# In spark SQL\n","\n","products_df.createOrReplaceTempView('products')\n","\n","spark.sql('''\n","SELECT DISTINCT product_id, 10 AS price\n","FROM Products\n","WHERE product_id NOT IN (\n","    SELECT DISTINCT product_id\n","    FROM Products\n","    WHERE change_date <= '2019-08-16'\n",")\n","UNION\n","SELECT product_id, new_price AS price\n","FROM Products\n","WHERE (product_id, change_date) IN (\n","    SELECT product_id, MAX(change_date)\n","    FROM Products\n","    WHERE change_date <= '2019-08-16'\n","    GROUP BY product_id\n",")\n","''').show()"],"metadata":{"ExecuteTime":{"end_time":"2023-11-05T19:24:19.556895Z","start_time":"2023-11-05T19:24:16.593417Z"},"id":"b05a2345639009cc","outputId":"22a1aa48-5631-4e9d-804e-5e1015772c56","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743290756775,"user_tz":-330,"elapsed":6143,"user":{"displayName":"Himanshu Singh","userId":"04013931203390313061"}}},"id":"b05a2345639009cc"},{"cell_type":"markdown","source":["# TAKE AWAY\n","## EXAMPLE SHOWING FAILURE OF\n","## `products_df['product_id'] or products_df.product_id`\n","\n","seeing some wiered behaviour while selecting\n","The error you're encountering when using products_df['product_id'] or products_df.product_id in the select statement is due to the way PySpark handles column references, particularly in joins."],"metadata":{"id":"fBiuf9jAhzUG"},"id":"fBiuf9jAhzUG"},{"cell_type":"code","source":["#In pyspark dataframe unkown error INGNORE THIS\n","subdf = products_df\\\n","             .where(col('change_date') <= '2019-08-16')\\\n","             .select(\"product_id\").distinct()\n","\n","df1 = products_df\\\n","    .join(subdf, on=\"product_id\", how=\"left_anti\")\\\n","    .select(\"product_id\").distinct()\\\n","    .withColumn(\"price\", lit(10))\n","\n","df1.show()\n","\n","\n","subdf=products_df\\\n","        .where(col('change_date') <= '2019-08-16')\\\n","        .groupBy('product_id').agg(max('change_date').alias('mx'))\n","\n","df2=products_df\\\n","        .join(subdf, (col('change_date')==col('mx')) & (products_df.product_id==subdf.product_id), 'inner')\\\n","        .select(products_df['product_id'],col('new_price').alias('price'))\n","df2.show()\n","\n","\n","df2=products_df\\\n","        .join(subdf, (col('change_date')==col('mx')) & (products_df.product_id==subdf.product_id), 'inner')\\\n","        .select(products_df.product_id,col('new_price').alias('price'))\n","df2.show()\n","\n","\n","\n","\"\"\"\n","The error you're encountering when using products_df['product_id'] or products_df.product_id in the select statement\n","is due to the way PySpark handles column references, particularly in joins.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738},"id":"S16JmDcheYy2","executionInfo":{"status":"error","timestamp":1743300795571,"user_tz":-330,"elapsed":1580,"user":{"displayName":"Himanshu Singh","userId":"04013931203390313061"}},"outputId":"3fcc1da0-cc5f-4fa0-9ce3-a3f4fc1e06fc"},"id":"S16JmDcheYy2","execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+-----+\n","|product_id|price|\n","+----------+-----+\n","|         3|   10|\n","+----------+-----+\n","\n"]},{"output_type":"error","ename":"AnalysisException","evalue":"Column product_id#0L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-a03217d94f64>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproducts_df\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'change_date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproducts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct_id\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0msubdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'product_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: Column product_id#0L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."]}]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["spark.stop()"],"metadata":{"ExecuteTime":{"end_time":"2023-11-05T19:24:20.462979900Z","start_time":"2023-11-05T19:24:19.550090500Z"},"id":"55e04ccfc562fbf2"},"id":"55e04ccfc562fbf2"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}